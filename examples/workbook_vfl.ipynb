{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertival Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "property_render_1"
    ]
   },
   "source": [
    "The easiest way to get started is to run the example on a hosted Python environment using Google Colab. To open the example on Google Colab click on the \"Open in Colab\" button below. If you chose Google Colab, you will need to set some variabales at the beginning of the notebook. The easiest way to do this is by clicking on the copy button below, which will copy all variables and insert them at the same position in the notebook on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "property_render_2"
    ]
   },
   "outputs": [],
   "source": [
    "PYTHON_VERSION = \"{PYTHON_VERSION}\"  # noqa: F821\n",
    "ARTIFACT_USER = \"{ARTIFACT_USER} \"  # noqa: F821\n",
    "ARTIFACT_KEY = \"{ARTIFACT_KEY}\"  # noqa: F821\n",
    "PYPI_REGISTRY = \"{PYPI_REGISTRY}\"  # noqa: F821\n",
    "ORGANIZATION_ID = \"{ORGANIZATION_ID}\"  # noqa: F821\n",
    "SERVER_ADDRESS = \"{SERVER_ADDRESS}\"  # noqa: F821\n",
    "TOKEN_URL = \"{TOKEN_URL}\"  # noqa: F821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/katulu-io/examples/blob/{PLATFORM_VERSION}/examples/workbook_vfl.ipynb)\n",
    "\n",
    "This example demonstrates how to use Vertical Federated Learning. The demo covers:\n",
    "* A short introduction to the concept of Vertical Federated Learning\n",
    "* A demonstration on how to install and onboard an agent\n",
    "* How to access the data and prepare it for training\n",
    "* The training of a model using federated learning\n",
    "* How to evaluate the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Vertical Federated Learning\n",
    "\n",
    "Vertical Federated Learning is a machine learning technique that allows multiple parties to train a model in a setting where each party has access to different features of the data. This allows the different parties to train a model without sharing their data with each other. For example if we want to train a model along the value chain, we will usually need data from different companies. In this case, each company trains a part of the model using their own data and the model is then virtually combined to create a global model. This is done by cutting a global model into two parts, one for each company, and only the output from one model is communicated to the other.\n",
    "\n",
    "One example for such a scenario is where we would like to the predict the quality of a product based on the raw materials used in the production. In this case, the raw material supplier has data on the quality of the raw materials and the manufacturer has data on the quality of the final product. Each dataset for itself contains limited value to improve the process. However, if the producer can use the data from the supplier without ever seeing it, she can produce higher quality with lower costs and both can participate in the value created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "agent_installation"
    ]
   },
   "source": [
    "## Installing the agent\n",
    "\n",
    "for this demo we will install the agents locally and use an already prepared dataset. \n",
    "\n",
    "We will run the agents directly in Python. Please make sure that you have at least Python > 10.14. installed. First, we will create a virtual environment called `platform_demo` and and activate it. If you don't want to use a virtual environment, you can skip this step.\n",
    "```bash\n",
    "python -m venv platform_demo\n",
    "source platform_demo/bin/activate\n",
    "```\n",
    "\n",
    "Next, we will install the required packages. \n",
    "```bash\n",
    "pip install katulu-agent=={PYTHON_VERSION} -U --extra-index-url https://{ARTIFACT_USER}:{ARTIFACT_KEY}@{PYPI_REGISTRY}\n",
    "```\n",
    "\n",
    "More details on agent installation and different options can be found in the [agent documentation](/docs/agent/installation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "download_quality_data"
    ]
   },
   "source": [
    "### Download the datasets \n",
    "\n",
    "After installing the agent we will need to download the datasets. This can be done with `gcloud CLI`. \n",
    "If you don't have `gcloud CLI` installed, you can install it by following the instructions [here](https://cloud.google.com/sdk/docs/install). \n",
    "\n",
    "Next, we will download the dataset and store it in a folder called `data`.\n",
    "\n",
    "```bash\n",
    "mkdir data\n",
    "echo {ARTIFACT_KEY} \\\n",
    "| base64 --decode | gcloud auth activate-service-account --key-file=-\n",
    "gcloud storage cp gs://demo-agent-data-files/* data/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "agent_creation"
    ]
   },
   "source": [
    "### Configure and start the agents\n",
    "\n",
    "Now we will create two agents on the Platform. To do this, we need to open the [agents page](/{ORGANIZATION_ID}/agents) and click on the `Create Agent` button. This will open a dialog where we need to set a name for the agent and can define some labels, to better identify the agent, e.g., location, hardware, etc. We will call the agents `agent_1` and `agent_2`. After inserting a name and maybe some labels, we will click `Create Agent`. This will open up a new page, where we can download the first part of the configuration file by clicking the `Download (agent_1.yml)` (respective `agent_2.yml`) button on the `Configuration File(...)` section.\n",
    "\n",
    "The configuration file should be place into the current directory and we will need to append the data specific configuration next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first agent, we will append the following information to the configuration file:\n",
    "```yaml\n",
    "datasets:\n",
    "  - name: chemical_passive\n",
    "      type:\n",
    "        parquet:\n",
    "          file: .//data/ungrouped_passive.parquet\n",
    "      collaboration:\n",
    "        - secret:\n",
    "            password:\n",
    "              value: \"12345\"\n",
    "      privacy_level: 0\n",
    "```\n",
    "\n",
    "and for the second agent:\n",
    "```yaml\n",
    "datasets:\n",
    "  - name: chemical_active\n",
    "      type:\n",
    "        parquet:\n",
    "          file: .//data/active.parquet\n",
    "      collaboration:\n",
    "        - secret:\n",
    "            password:\n",
    "              value: \"12345\"\n",
    "      privacy_level: 0\n",
    "```\n",
    "\n",
    "This will tell the agent to use the data from the `ungrouped_passive.parquet` and `active.parquet` files. The `colaboration` section is used to define a key that the agents use to encrypt identifiers. The server needs to run a private set intersection protocol to find the common identifiers between the agents. To keep the identifiers private they are first ecrypted through password based encryption (PBKDF2 HMAC + PRF blake2b) on the agents, before they are shared with the server. To ensure that the encrypted ids are the same, the agents need to use the same pre-shared password.\n",
    "\n",
    "The `privacy_level` is set to `0` which means that the we will not use any additional privacy preserving techniques on the data. In a real world scenario, you would want to use a higher privacy level, e.g., `1` or `2` to ensure that the data cannot be reconstructed from the model.\n",
    "\n",
    "The full configuration file should look like this:\n",
    "```yaml\n",
    "id: {AGENT_ID}\n",
    "server_url: https://{SERVER_ADDRESS}\n",
    "credentials:\n",
    "    token_url: {TOKEN_URL}\n",
    "    client_id: {CLIENT_ID}\n",
    "    client_secret: {CLIENT_SECRET}\n",
    "datasets:\n",
    "  - name: chemical_{active/passive}\n",
    "      type:\n",
    "        parquet:\n",
    "          file: .//data/{active/ungrouped_passive}.parquet\n",
    "      collaboration:\n",
    "        - secret:\n",
    "            password:\n",
    "              value: \"12345\"\n",
    "      privacy_level: 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "agent_start"
    ]
   },
   "source": [
    "More information on the configuration file can be found in the [agent documentation](/docs/agent/configuration).\n",
    "\n",
    "Now we are all set to start the agents. We can do this by opening two terminals and run the following command in the terminals:\n",
    "```bash\n",
    "source platform_demo/bin/activate\n",
    "katulu-agent agent_1.yml\n",
    "```\n",
    "and\n",
    "```bash\n",
    "source platform_demo/bin/activate\n",
    "katulu-agent agent_2.yml\n",
    "``` \n",
    "\n",
    "The agents are sucessfully started when you see three lines in the terminal starting with:\n",
    "```\n",
    "Starting agent\n",
    "Retrieving server version\n",
    "Schemas registered with server \n",
    "```\n",
    "\n",
    "If you are not seeing these lines, please check the configuration file and the [troubleshooting guide](/docs/agent/troubleshooting).\n",
    "\n",
    "Now we can move to the next step and explore the data and prepare it for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sdk_setup_1"
    ]
   },
   "source": [
    "## Exploring the SDK\n",
    "\n",
    "To interact with the Platform we will use the Katulu SDK. The SDK is a Python package that provides an easy way to interact with the Platform. The SDK can be installed by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sdk_setup_2"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install katulu-sdk=={PYTHON_VERSION} -U --extra-index-url https://download.pytorch.org/whl/cpu --extra-index-url https://{ARTIFACT_USER}:{ARTIFACT_KEY}@{PYPI_REGISTRY}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sdk_setup_3"
    ]
   },
   "source": [
    "You can write the SDK code in a Python script and call it with `python script.py` or you can use a Jupyter notebook. For this demo we will use a Jupyter notebook. You can use a hosted Jupyter service like, Google Colab or Kaggle Notebook. We will use a local installation in this demo. To start a Jupyter notebook, run the following command:\n",
    "```bash\n",
    "pip install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "This installs and starts a Jupyter notebook server. You can now open a browser and navigate to `http://localhost:8888` to open the Jupyter notebook interface. There you can create a new notebook and start writing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook setup\n",
    "\n",
    "First we will import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: PLE1142\n",
    "import torch\n",
    "\n",
    "from katulu.sdk import (\n",
    "    Adam,\n",
    "    BinaryAccuracy,\n",
    "    BinaryCrossEntropyWLogitsLoss,\n",
    "    JobSpecConfig,\n",
    "    VerticalJobSpec,\n",
    "    build_vfl_job_spec,\n",
    "    connect,\n",
    "    model_from_torch,\n",
    ")\n",
    "from katulu.sdk.pipeline import (\n",
    "    Alias,\n",
    "    Avg,\n",
    "    Cast,\n",
    "    CastType,\n",
    "    Features,\n",
    "    GroupBy,\n",
    "    Max,\n",
    "    MinMaxScaler,\n",
    "    PSIJoinOn,\n",
    "    Select,\n",
    "    Source,\n",
    "    Targets,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chemical_project_setup"
    ]
   },
   "source": [
    "### Project setup\n",
    "\n",
    "Now we will need to create a new project on the Platform. To do this, we need to open the [projects page](/{ORGANIZATION_ID}/projects) and click on the `Create Project` button. This will open a dialog where we need to set a name for the project. We will call the project `chemical_quality_prediction`. After inserting the name, we will click `Create Project`. This will open up a new page, and it will show the `PROJECT_ID` in the top of the page, next the project name. We will need this `PROJECT_ID` to connect to the project in the SDK. We can insert it into the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "client_parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"{PROJECT_ID}\"  # noqa: F821\n",
    "CLIENT_ID = \"{CLIENT_ID}\"  # noqa: F821\n",
    "CLIENT_SECRET = \"{CLIENT_SECRET}\"  # noqa: F821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chemical_client_setup"
    ]
   },
   "source": [
    "### Client setup\n",
    "\n",
    "To get the `CLIENT_ID` and `CLIENT_SECRET`, we will create new Access Credential associated to our profile. Open the [Access Credentials page](/profile/access-credentials) and click on the `Create Access Credential` button. This will open a dialog where we need to set a name for the credential. We will call the credential `chemical_quality_credentials`. After inserting the name, we will click `Create Access Credential`. This will open up a new page, and it will show all the information we need to fill in the missing pieces in the above code.\n",
    "\n",
    "These are all the required information to connect to the Platform. Now we can start exploring the data and prepare it for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the dataset names that we want to retrieve from the Platform. The name should be the same as the name in the configuration file. In this case, we will use `chemical_active` and `chemical_passive` and create an empty dictionary to store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"chemical_active\", \"chemical_passive\"]\n",
    "datasets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can connect to the Platform and retrieve the data and print their schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:04.172445Z",
     "iopub.status.busy": "2024-07-04T12:18:04.172188Z",
     "iopub.status.idle": "2024-07-04T12:18:04.187604Z",
     "shell.execute_reply": "2024-07-04T12:18:04.186975Z"
    },
    "tags": [
     "get_data"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-07-15 08:47:03\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mIssuing new access token      \u001b[0m \u001b[36msource\u001b[0m=\u001b[35mkatulu.core.auth.jwt\u001b[0m\n",
      "Found dataset: d84d1772da1373ad9d919a18762ef37d08f014d89502d2d343aeb1edf35b8abd\n",
      "╒═════════╤════════╕\n",
      "│ Field   │ Type   │\n",
      "╞═════════╪════════╡\n",
      "│ quality │ Int64  │\n",
      "├─────────┼────────┤\n",
      "│ id      │ Int64  │\n",
      "╘═════════╧════════╛\n",
      "Found dataset: 69d9977aee4b2f18687aca9f0bc6f2c3b19ffc91523bf480c26cd797c7d793ef\n",
      "╒══════════════════════╤═════════╕\n",
      "│ Field                │ Type    │\n",
      "╞══════════════════════╪═════════╡\n",
      "│ fixed acidity        │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ volatile acidity     │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ citric acid          │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ residual sugar       │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ chlorides            │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ free sulfur dioxide  │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ total sulfur dioxide │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ density              │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ pH                   │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ sulphates            │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ alcohol              │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ id                   │ Int64   │\n",
      "╘══════════════════════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "# Connect to the Platform to get the dataset\n",
    "async with connect(\n",
    "    project_id=PROJECT_ID,\n",
    "    organization_id=ORGANIZATION_ID,\n",
    "    server_address=SERVER_ADDRESS,\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    token_url=TOKEN_URL,\n",
    ") as session:\n",
    "    for dataset_name in DATASET_NAMES:\n",
    "        ds = (await session.find_sources(name=dataset_name))[0]\n",
    "        datasets[dataset_name] = ds\n",
    "        print(f\"Found dataset: {ds.id}\")\n",
    "        print(ds.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Schema contains the available columns and their types. We can use this information to prepare the data for training.\n",
    "\n",
    "To do this we define which columns we want to use for training and which column we want to predict. In this case, we will use all columns except the `quality` column to train the model and we will use the `quality` column to predict the quality of the chemical solution. The features will be put into a list called `fields` and the target column will be stored in a list called `target`. Additionally, we will need to define a `psi_column` which is used to match the data between the agents. In this case, we will use the `id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    \"fixed acidity\",\n",
    "    \"volatile acidity\",\n",
    "    \"citric acid\",\n",
    "    \"residual sugar\",\n",
    "    \"chlorides\",\n",
    "    \"free sulfur dioxide\",\n",
    "    \"total sulfur dioxide\",\n",
    "    \"density\",\n",
    "    \"pH\",\n",
    "    \"sulphates\",\n",
    "    \"alcohol\",\n",
    "]\n",
    "\n",
    "target = [\"quality\"]\n",
    "psi_column = [\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline defines the data transformations that needs to be applied to the data. In this case, we know fromt the production that they take multiple mesurements per batch. Therefore, we will first group the data by the batch id and aggregate the values by the `MAX` function. This will give us a single row per batch with the maximum value of each feature.\n",
    "\n",
    "In the vertical federated learning setting, we need to define two pipelines. One for the passive agent and one for the active agent. The active agent will have the `quality` column and the `id` column and provides the targets for the training, while the passive agent will have all the other columns and provides the features.\n",
    "\n",
    "The active pipeline will define the `PSIJoinOn` column and the `Targets`. The `Targets` is the `quality` value casted to a `Float32`. \n",
    "\n",
    "The passive pipeline will define the `PSIJoinOn` column and the `Features`. The `Features` are all columns except the `quality` column. The `Features` are casted to `Float32` and normalized. We also knwo from production that they take multiple measurements per batch. Therefore, we will first group the data by the batch id and aggregate the values by the `MAX` function. This will give us a single row per batch with the maximum value of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:04.225189Z",
     "iopub.status.busy": "2024-07-04T12:18:04.224903Z",
     "iopub.status.idle": "2024-07-04T12:18:04.231098Z",
     "shell.execute_reply": "2024-07-04T12:18:04.230392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "# Define an active and a passive pipeline\n",
    "# fmt: off\n",
    "active_pipeline = (\n",
    "    Source(datasets[\"chemical_active\"])\n",
    "    + Targets(Select(target) | Cast(target, CastType.Float32))\n",
    "    + PSIJoinOn(Select(psi_column))\n",
    ")\n",
    "\n",
    "passive_pipeline = (\n",
    "    Source(datasets[\"chemical_passive\"])\n",
    "    | GroupBy([\"id\"], [Alias(Max(f), f) for f in fields])\n",
    "    + PSIJoinOn(Select(psi_column))\n",
    "    + Features(Select(fields) + Cast(fields, CastType.Float32) + MinMaxScaler(fields))\n",
    ")\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define two simple models that we will train on the data. \n",
    "\n",
    "The passive model will accept the features as its inputs and is a simple feed forward neural network with two hidden layers and no output. The output from the passive model is the input to the active model, and the active model will predict the quality of the chemical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:04.233458Z",
     "iopub.status.busy": "2024-07-04T12:18:04.233251Z",
     "iopub.status.idle": "2024-07-04T12:18:04.237703Z",
     "shell.execute_reply": "2024-07-04T12:18:04.237140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the models\n",
    "PASSIVE_MODEL = torch.nn.Sequential(\n",
    "    torch.nn.BatchNorm1d(len(fields)),\n",
    "    torch.nn.Linear(len(fields), 20),\n",
    "    torch.nn.Linear(20, 20),\n",
    "    torch.nn.ReLU(),\n",
    ")\n",
    "\n",
    "ACTIVE_MODEL = torch.nn.Sequential(\n",
    "    torch.nn.Linear(20, 20),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.LayerNorm(20),\n",
    "    torch.nn.Linear(20, 20),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnaly, we put all the pieces together and define a training `Job`. It contains the `pipeline`, the two models, and a configuration for the training.\n",
    "\n",
    "We will train the model for 5 epochs with a batch size of 256. For the optimizer, we will use the `Adam` optimizer with a learning rate of `0.001`. We will use the `BinarCrossEntropyLoss` as the loss function, as we want the predict if the quality of the chemical solution meets some binary quality criteria.\n",
    "\n",
    "As a metric to track during the training we use the `Accuracy` metric. This metric will tell us how many of the predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:04.240197Z",
     "iopub.status.busy": "2024-07-04T12:18:04.239928Z",
     "iopub.status.idle": "2024-07-04T12:18:04.354366Z",
     "shell.execute_reply": "2024-07-04T12:18:04.353672Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the job spec\n",
    "job_spec = build_vfl_job_spec(\n",
    "    name=\"chemical_vfl\",\n",
    "    passive=VerticalJobSpec(\n",
    "        pipeline=passive_pipeline,\n",
    "        model=model_from_torch(PASSIVE_MODEL),\n",
    "    ),\n",
    "    active=VerticalJobSpec(\n",
    "        pipeline=active_pipeline,\n",
    "        model=model_from_torch(ACTIVE_MODEL),\n",
    "    ),\n",
    "    config=JobSpecConfig(\n",
    "        num_rounds=5,\n",
    "        batch_size=256,\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        loss_function=BinaryCrossEntropyWLogitsLoss(),\n",
    "        metrics=[BinaryAccuracy()],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are all set and can start the training job. We will print the training progress and also have the chance to track the training metrics on the platform by following the link in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:04.357360Z",
     "iopub.status.busy": "2024-07-04T12:18:04.356974Z",
     "iopub.status.idle": "2024-07-04T12:18:31.586860Z",
     "shell.execute_reply": "2024-07-04T12:18:31.583449Z"
    },
    "tags": [
     "submit_job"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-07-15 08:47:16\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mIssuing new access token      \u001b[0m \u001b[36msource\u001b[0m=\u001b[35mkatulu.core.auth.jwt\u001b[0m\n",
      "Job ID: a012baf0-49eb-4d67-b540-b5a29afd6c58\n",
      "https://https://platform.katulu.io/ca2bf2b8-3cd1-427b-8b39-c1b09c72c0ed/bcd538b2-422f-400e-a9fd-18aa54f92ec5/jobs\n",
      "Round: 1 {'accuracy': 0.6272125593401641}\n",
      "Round: 2 {'accuracy': 0.5276281356591342}\n",
      "Round: 3 {'accuracy': 0.5831922422523992}\n",
      "Round: 4 {'accuracy': 0.6041249804484288}\n",
      "Round: 5 {'accuracy': 0.621517623206625}\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "async with connect(\n",
    "    project_id=PROJECT_ID,\n",
    "    organization_id=ORGANIZATION_ID,\n",
    "    server_address=SERVER_ADDRESS,\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    token_url=TOKEN_URL,\n",
    ") as session:\n",
    "    job_id = await session.fit(job_spec)  # noqa: PLE1142\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print(f\"/{ORGANIZATION_ID}/{PROJECT_ID}/jobs\")\n",
    "    round = 1\n",
    "    async for metrics in session.metrics(job_id):\n",
    "        print(\"Round:\", round, {m.name: metrics.items[f\"val_{m.name}\"] for m in job_spec.config.metrics})\n",
    "        round += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen from the training that the model wasn't able to learn the data. We possibily made a mistake in the data preperation. Therefore, we will go back and check the pipeline. We will change the aggregation function from `MAX` to `MEAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:31.601779Z",
     "iopub.status.busy": "2024-07-04T12:18:31.599720Z",
     "iopub.status.idle": "2024-07-04T12:18:31.616407Z",
     "shell.execute_reply": "2024-07-04T12:18:31.614502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use MEAN aggregation\n",
    "# fmt: off\n",
    "active_pipeline = (\n",
    "    Source(datasets[\"chemical_active\"])\n",
    "    + Targets(Select(target) | Cast(target, CastType.Float32))\n",
    "    + PSIJoinOn(Select([\"id\"]))\n",
    ")\n",
    "\n",
    "passive_pipeline = (\n",
    "    Source(datasets[\"chemical_passive\"])\n",
    "    | GroupBy([\"id\"], [Alias(Avg(f), f) for f in fields])\n",
    "    + PSIJoinOn(Select(psi_column))\n",
    "    + Features(Select(fields) + Cast(fields, CastType.Float32) + MinMaxScaler(fields))\n",
    ")\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new Job with the updated pipeline and start the training. This time the model should be able to learn the data and we should see the accuracy increasing over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:31.622179Z",
     "iopub.status.busy": "2024-07-04T12:18:31.621744Z",
     "iopub.status.idle": "2024-07-04T12:18:31.646218Z",
     "shell.execute_reply": "2024-07-04T12:18:31.645066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the job spec\n",
    "job_spec = build_vfl_job_spec(\n",
    "    name=\"chemical_vfl\",\n",
    "    passive=VerticalJobSpec(\n",
    "        pipeline=passive_pipeline,\n",
    "        model=model_from_torch(PASSIVE_MODEL),\n",
    "    ),\n",
    "    active=VerticalJobSpec(\n",
    "        pipeline=active_pipeline,\n",
    "        model=model_from_torch(ACTIVE_MODEL),\n",
    "    ),\n",
    "    config=JobSpecConfig(\n",
    "        num_rounds=5,\n",
    "        batch_size=256,\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        loss_function=BinaryCrossEntropyWLogitsLoss(),\n",
    "        metrics=[BinaryAccuracy(threshold=0.0)],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:31.649238Z",
     "iopub.status.busy": "2024-07-04T12:18:31.649012Z",
     "iopub.status.idle": "2024-07-04T12:18:57.735290Z",
     "shell.execute_reply": "2024-07-04T12:18:57.731206Z"
    },
    "tags": [
     "submit_job"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-07-15 08:50:32\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mIssuing new access token      \u001b[0m \u001b[36msource\u001b[0m=\u001b[35mkatulu.core.auth.jwt\u001b[0m\n",
      "Job ID: 04a2dcd4-f555-4404-97de-2fd608afead6\n",
      "https://https://platform.katulu.io/ca2bf2b8-3cd1-427b-8b39-c1b09c72c0ed/bcd538b2-422f-400e-a9fd-18aa54f92ec5/jobs\n",
      "Round: 1 {'accuracy': 0.6647683544050313}\n",
      "Round: 2 {'accuracy': 0.6941665382096817}\n",
      "Round: 3 {'accuracy': 0.7163306139919857}\n",
      "Round: 4 {'accuracy': 0.7249499768023309}\n",
      "Round: 5 {'accuracy': 0.7249499768023309}\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "async with connect(\n",
    "    project_id=PROJECT_ID,\n",
    "    organization_id=ORGANIZATION_ID,\n",
    "    server_address=SERVER_ADDRESS,\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    token_url=TOKEN_URL,\n",
    ") as session:\n",
    "    job_id = await session.fit(job_spec)  # noqa: PLE1142\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print(f\"/{ORGANIZATION_ID}/{PROJECT_ID}/jobs\")\n",
    "    round = 1\n",
    "    async for metrics in session.metrics(job_id):\n",
    "        print(\"Round:\", round, {m.name: metrics.items[f\"val_{m.name}\"] for m in job_spec.config.metrics})\n",
    "        round += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that the Accuracy increased by more then 10% and the model is able to predict the quality of the chemical solution with an accuracy of around 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the demo we have seen how to install and onboard agents, how to access the data and prepare it for training, how to train a model using federated learning and how to evaluate the model. Now it is time to explore the platform and try it out yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}