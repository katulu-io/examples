{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizontal Federated Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "property_render_1"
    ]
   },
   "source": [
    "The easiest way to get started is to run the example on a hosted Python environment using Google Colab. To open the example on Google Colab click on the \"Open in Colab\" button below. If you chose Google Colab, you will need to set some variabales at the beginning of the notebook. The easiest way to do this is by clicking on the copy button below, which will copy all variables and insert them at the same position in the notebook on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "property_render_2"
    ]
   },
   "outputs": [],
   "source": [
    "PYTHON_VERSION = \"{PYTHON_VERSION}\"  # noqa: F821\n",
    "ARTIFACT_USER = \"{ARTIFACT_USER} \"  # noqa: F821\n",
    "ARTIFACT_KEY = \"{ARTIFACT_KEY}\"  # noqa: F821\n",
    "PYPI_REGISTRY = \"{PYPI_REGISTRY}\"  # noqa: F821\n",
    "ORGANIZATION_ID = \"{ORGANIZATION_ID}\"  # noqa: F821\n",
    "SERVER_ADDRESS = \"{SERVER_ADDRESS}\"  # noqa: F821\n",
    "TOKEN_URL = \"{TOKEN_URL}\"  # noqa: F821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/katulu-io/examples/blob/v{DOCKER_VERSION}/examples/workbook_hfl.ipynb)\n",
    "\n",
    "\n",
    "This is a demonstration of Horizontal Federated Learning using the Katulu platform. In this demo we will:\n",
    "* Provide a short introduction to the concept of Horizontal Federated Learning\n",
    "* Demonstrate how to install and onboard an agent\n",
    "* Show how to access the data and prepare it for training\n",
    "* Explain how to train a model using federated learning\n",
    "* Examplify how to evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro Horizontal Federated Learning\n",
    "\n",
    "Federated Learning is a machine learning setting where many agents (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. Federated Learning is great for privacy and security reasons, as the data never leaves the agent device. Horizontal Federated Learning is a specific type of federated learning where the data is distributed across the agents and all agents can provide the same features. From a practical perspective, this means that all agents' data is associated to the same process.\n",
    "\n",
    "In an industrial context this process could be predicting the quality of a chemical solution based on different measurements within a factory. Each factory can be an agent and we want to train a model to predict the quality based on all data collected from a global production. With federated learning, we can train a model that is able to predict the quality without sharing any data between the factorys.\n",
    "\n",
    "For this demo we will focuse on the above use case but we could also use federated learing for quality prediction in other processes, e.g., like PCB production where we want to predict the PCB' quality at the end of the line based on the process data. Or for other cases like predictive maintenance, where we want to predict the remaining lifetime of a machine based on the data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "agent_installation"
    ]
   },
   "source": [
    "## Installing the agent\n",
    "\n",
    "for this demo we will install the agents locally and use an already prepared dataset. \n",
    "\n",
    "We will run the agents directly in Python. Please make sure that you have at least Python > 10.14. installed. First, we will create a virtual environment called `platform_demo` and and activate it. If you don't want to use a virtual environment, you can skip this step.\n",
    "```bash\n",
    "python -m venv platform_demo\n",
    "source platform_demo/bin/activate\n",
    "```\n",
    "\n",
    "Next, we will install the required packages. \n",
    "```bash\n",
    "pip install katulu-agent=={PYTHON_VERSION} -U --extra-index-url https://download.pytorch.org/whl/cpu --extra-index-url https://{ARTIFACT_USER}:{ARTIFACT_KEY}@{PYPI_REGISTRY}\n",
    "```\n",
    "\n",
    "More details on agent installation and different options can be found in the [agent documentation]({SERVER_URL}/docs/agent/installation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "download_quality_data"
    ]
   },
   "source": [
    "### Download the datasets \n",
    "\n",
    "After installing the agent we will need to download the datasets. This can be done with `gcloud CLI`. \n",
    "If you don't have `gcloud CLI` installed, you can install it by following the instructions [here](https://cloud.google.com/sdk/docs/install). \n",
    "\n",
    "Next, we will download the dataset and store it in a folder called `data`.\n",
    "\n",
    "```bash\n",
    "mkdir data\n",
    "echo {ARTIFACT_KEY} \\\n",
    "| base64 --decode | gcloud auth activate-service-account --key-file=-\n",
    "gcloud storage cp gs://demo-agent-data-files/* data/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "agent_creation"
    ]
   },
   "source": [
    "### Configure and start the agents\n",
    "\n",
    "Now we will create two agents on the Platform. To do this, we need to open the [agents page]({{SERVER_URL}}/{{ORGANIZATION_ID}}/agents) and click on the `Create Agent` button. This will open a dialog where we need to set a name for the agent and can define some labels, to better identify the agent, e.g., location, hardware, etc. We will call the agents `agent_1` and `agent_2`. After inserting a name and maybe some labels, we will click `Create Agent`. This will open up a new page, where we can download the first part of the configuration file by clicking the `Download (agent_1.yml)` (respective `agent_2.yml`) button on the `Configuration File(...)` section.\n",
    "\n",
    "The configuration file should be place into the current directory and we will need to append the data specific configuration next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first agent, we will add the following information to the configuration file:\n",
    "```yaml\n",
    "datasets:\n",
    "  - name: chemical\n",
    "      type:\n",
    "        parquet:\n",
    "          file: .//data/agent_1.parquet\n",
    "      privacy_level: 0\n",
    "```\n",
    "\n",
    "and for the second agent:\n",
    "```yaml\n",
    "datasets:\n",
    "  - name: chemical\n",
    "      type:\n",
    "        parquet:\n",
    "          file: .//data/agent_2.parquet\n",
    "      privacy_level: 0\n",
    "```\n",
    "\n",
    "This will tell the agent to use the data from the `agent_1.parquet` and `agent_2.parquet` files. The `privacy_level` is set to `0` which means that the we will not use any additional privacy preserving techniques on the data. In a real world scenario, you would want to use a higher privacy level, e.g., `1` or `2` to ensure that the data cannot be reconstructed from the model.\n",
    "\n",
    "The full configuration file should look like this:\n",
    "```yaml\n",
    "id: {AGENT_ID}\n",
    "server_url: {SERVER_URL}\n",
    "credentials:\n",
    "    token_url: {TOKEN_URL}\n",
    "    client_id: {CLIENT_ID}\n",
    "    client_secret: {CLIENT_SECRET}\n",
    "datasets:\n",
    "  - name: chemical\n",
    "      type:\n",
    "        parquet:\n",
    "          file: .//data/agent_{agent_number}.parquet\n",
    "      privacy_level: 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "agent_start"
    ]
   },
   "source": [
    "More information on the configuration file can be found in the [agent documentation]({SERVER_URL}/docs/agent/configuration).\n",
    "\n",
    "Now we are all set to start the agents. We can do this by opening two terminals and run the following command in the terminals:\n",
    "```bash\n",
    "source platform_demo/bin/activate\n",
    "katulu-agent agent_1.yml\n",
    "```\n",
    "and\n",
    "```bash\n",
    "source platform_demo/bin/activate\n",
    "katulu-agent agent_2.yml\n",
    "``` \n",
    "\n",
    "The agents are sucessfully started when you see three lines in the terminal starting with:\n",
    "```\n",
    "Starting agent\n",
    "Retrieving server version\n",
    "Schemas registered with server \n",
    "```\n",
    "\n",
    "If you are not seeing these lines, please check the configuration file and the [troubleshooting guide]({SERVER_URL}/docs/agent/troubleshooting).\n",
    "\n",
    "Now we can move to the next step and explore the data and prepare it for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sdk_setup_1"
    ]
   },
   "source": [
    "## Exploring the SDK\n",
    "\n",
    "To interact with the Platform we will use the Katulu SDK. The SDK is a Python package that provides an easy way to interact with the Platform. The SDK can be installed by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "sdk_setup_2"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install katulu-sdk=={PYTHON_VERSION} -U --extra-index-url https://download.pytorch.org/whl/cpu --extra-index-url https://{ARTIFACT_USER}:{ARTIFACT_KEY}@{PYPI_REGISTRY}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "sdk_setup_3"
    ]
   },
   "source": [
    "You can write the SDK code in a Python script and call it with `python script.py` or you can use a Jupyter notebook. For this demo we will use a Jupyter notebook. You can use a hosted Jupyter service like, Google Colab or Kaggle Notebook. We will use a local installation in this demo. To start a Jupyter notebook, run the following command:\n",
    "```bash\n",
    "pip install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "This installs and starts a Jupyter notebook server. You can now open a browser and navigate to `http://localhost:8888` to open the Jupyter notebook interface. There you can create a new notebook and start writing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "notebook_setup"
    ]
   },
   "source": [
    "### Notebook setup.\n",
    "\n",
    "First we will import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:02.260194Z",
     "iopub.status.busy": "2024-07-04T12:18:02.258788Z",
     "iopub.status.idle": "2024-07-04T12:18:04.169819Z",
     "shell.execute_reply": "2024-07-04T12:18:04.169185Z"
    }
   },
   "outputs": [],
   "source": [
    "# ruff: noqa: PLE1142\n",
    "import torch\n",
    "\n",
    "from katulu.sdk import (\n",
    "    Adam,\n",
    "    BinaryAccuracy,\n",
    "    BinaryCrossEntropyWLogitsLoss,\n",
    "    JobSpecConfig,\n",
    "    build_job_spec,\n",
    "    connect,\n",
    "    model_from_torch,\n",
    ")\n",
    "from katulu.sdk.pipeline import (\n",
    "    Alias,\n",
    "    Avg,\n",
    "    Cast,\n",
    "    CastType,\n",
    "    Features,\n",
    "    GroupBy,\n",
    "    Max,\n",
    "    MinMaxScaler,\n",
    "    Select,\n",
    "    Source,\n",
    "    Targets,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chemical_project_setup"
    ]
   },
   "source": [
    "### Project setup\n",
    "\n",
    "Now we will need to create a new project on the Platform. To do this, we need to open the [projects page]({SERVER_URL}/{ORGANIZATION_ID}/projects) and click on the `Create Project` button. This will open a dialog where we need to set a name for the project. We will call the project `chemical_quality_prediction`. After inserting the name, we will click `Create Project`. This will open up a new page, and it will show the `PROJECT_ID` in the top of the page, next the project name. We will need this `PROJECT_ID` to connect to the project in the SDK. We can insert it into the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "client_parameters"
    ]
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"{PROJECT_ID}\"  # noqa: F821\n",
    "CLIENT_ID = \"{CLIENT_ID}\"  # noqa: F821\n",
    "CLIENT_SECRET = \"{CLIENT_SECRET}\"  # noqa: F821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "chemical_client_setup"
    ]
   },
   "source": [
    "### Client setup\n",
    "\n",
    "To get the `CLIENT_ID` and `CLIENT_SECRET`, we will create new Access Credential associated to our profile. Open the [Access Credentials page]({SERVER_URL}/{ORGANIZATION_ID}/access-credentials) and click on the `Create Access Credential` button. This will open a dialog where we need to set a name for the credential. We will call the credential `chemical_quality_credentials`. After inserting the name, we will click `Create Access Credential`. This will open up a new page, and it will show all the information we need to fill in the missing pieces in the above code.\n",
    "\n",
    "These are all the required information to connect to the Platform. Now we can start exploring the data and prepare it for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the dataset names that we want to retrieve from the Platform. The name should be the same as the name in the configuration file. In this case, we will use `chemical` and create an empty dictionary to store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAMES = [\"chemical\"]\n",
    "datasets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we connect to the Platfom, retrieve the data and print the schema of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:04.172445Z",
     "iopub.status.busy": "2024-07-04T12:18:04.172188Z",
     "iopub.status.idle": "2024-07-04T12:18:04.187604Z",
     "shell.execute_reply": "2024-07-04T12:18:04.186975Z"
    },
    "tags": [
     "get_data"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-07-15 08:44:59\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mIssuing new access token      \u001b[0m \u001b[36msource\u001b[0m=\u001b[35mkatulu.core.auth.jwt\u001b[0m\n",
      "Found dataset: 9b2d032cd1c1aedf101581e0bdb25ed5581110e5aad7daad4f752cebf26723ec\n",
      "╒══════════════════════╤═════════╕\n",
      "│ Field                │ Type    │\n",
      "╞══════════════════════╪═════════╡\n",
      "│ fixed acidity        │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ volatile acidity     │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ citric acid          │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ residual sugar       │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ chlorides            │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ free sulfur dioxide  │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ total sulfur dioxide │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ density              │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ pH                   │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ sulphates            │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ alcohol              │ Float64 │\n",
      "├──────────────────────┼─────────┤\n",
      "│ id                   │ Int64   │\n",
      "├──────────────────────┼─────────┤\n",
      "│ quality              │ Int64   │\n",
      "╘══════════════════════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "# Connect to the Platform to get the dataset\n",
    "async with connect(\n",
    "    project_id=PROJECT_ID,\n",
    "    organization_id=ORGANIZATION_ID,\n",
    "    server_address=SERVER_ADDRESS,\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    token_url=TOKEN_URL,\n",
    ") as session:\n",
    "    for dataset_name in DATASET_NAMES:\n",
    "        ds = (await session.find_sources(name=dataset_name))[0]\n",
    "        datasets[dataset_name] = ds\n",
    "        print(f\"Found dataset: {ds.id}\")\n",
    "        print(ds.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Schema contains the available columns and their types. We can use this information to prepare the data for training.\n",
    "\n",
    "To do this we define which columns we want to use for training and which column we want to predict. In this case, we will use all columns except the `quality` column to train the model and we will use the `quality` column to predict the quality of the chemical solution. The features will be put into a list called `fields` and the target column will be stored in a list called `target`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    \"fixed acidity\",\n",
    "    \"volatile acidity\",\n",
    "    \"citric acid\",\n",
    "    \"residual sugar\",\n",
    "    \"chlorides\",\n",
    "    \"free sulfur dioxide\",\n",
    "    \"total sulfur dioxide\",\n",
    "    \"density\",\n",
    "    \"pH\",\n",
    "    \"sulphates\",\n",
    "    \"alcohol\",\n",
    "]\n",
    "\n",
    "target = [\"quality\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline defines the data transformations that need to be applied to the data. In this case, we know from the production that they take multiple mesurements per batch. Therefore, we will first group the data by the batch id and aggregate the values by the `MAX` function. This will give us a single row per batch with the maximum value of each feature.\n",
    "\n",
    "Then, we define the `Targets` (what we want to predic) and cast them to a `Float32` type. We will do the same for the `Features` and cast them to a `Float32` type as well. We will also normalize the data by using the `MinMaxScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:04.225189Z",
     "iopub.status.busy": "2024-07-04T12:18:04.224903Z",
     "iopub.status.idle": "2024-07-04T12:18:04.231098Z",
     "shell.execute_reply": "2024-07-04T12:18:04.230392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "# fmt: off\n",
    "pipeline = (\n",
    "    Source(datasets[DATASET_NAMES[0]])\n",
    "    | GroupBy([\"id\"], [Alias(Max(f), f) for f in fields + target])\n",
    "    + Targets(Select(target) + Cast(target, CastType.Float32))\n",
    "    + Features(Select(fields) + Cast(fields, CastType.Float32) + MinMaxScaler(fields))\n",
    ")\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define a simple model that we will train on the data. The model will be a simple feed forward neural network with two hidden layers. The input size will be the number of features and the output size will be the number of targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:04.233458Z",
     "iopub.status.busy": "2024-07-04T12:18:04.233251Z",
     "iopub.status.idle": "2024-07-04T12:18:04.237703Z",
     "shell.execute_reply": "2024-07-04T12:18:04.237140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "MODEL = torch.nn.Sequential(\n",
    "    torch.nn.BatchNorm1d(len(fields)),\n",
    "    torch.nn.Linear(len(fields), 20),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.LayerNorm(20),\n",
    "    torch.nn.Linear(20, 20),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnaly, we put all the pieces together and define a training `Job`. It contains the `pipeline`, the `model`, and a configuration for the training.\n",
    "\n",
    "We will train the model for 30 epochs with a batch size of 256. For the optimizer, we will use the `Adam` optimizer with a learning rate of `0.001`. We will use the `BinarCrossEntropyLoss` as the loss function, as we want the predict if the quality of the chemical solution meets some binary quality criteria.\n",
    "\n",
    "As a metric to track during the training we use the `Accuracy` metric. This metric will tell us how many of the predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:04.240197Z",
     "iopub.status.busy": "2024-07-04T12:18:04.239928Z",
     "iopub.status.idle": "2024-07-04T12:18:04.354366Z",
     "shell.execute_reply": "2024-07-04T12:18:04.353672Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the job spec\n",
    "job_spec = build_job_spec(\n",
    "    name=\"chemical\",\n",
    "    pipeline=pipeline,\n",
    "    model=model_from_torch(MODEL),\n",
    "    config=JobSpecConfig(\n",
    "        num_rounds=30,\n",
    "        batch_size=256,\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        loss_function=BinaryCrossEntropyWLogitsLoss(),\n",
    "        metrics=[BinaryAccuracy(threshold=0.0)],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are all set and can start the training job. We will print the training progress and also have the chance to track the training metrics on the platform by following the link in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:04.357360Z",
     "iopub.status.busy": "2024-07-04T12:18:04.356974Z",
     "iopub.status.idle": "2024-07-04T12:18:31.586860Z",
     "shell.execute_reply": "2024-07-04T12:18:31.583449Z"
    },
    "tags": [
     "submit_job"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-07-15 08:45:10\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mIssuing new access token      \u001b[0m \u001b[36msource\u001b[0m=\u001b[35mkatulu.core.auth.jwt\u001b[0m\n",
      "Job ID: acee832f-5de7-4878-9c98-4e685d5041b6\n",
      "https://https://platform.katulu.io/ca2bf2b8-3cd1-427b-8b39-c1b09c72c0ed/bcd538b2-422f-400e-a9fd-18aa54f92ec5/jobs\n",
      "Round: 1 {'accuracy': 0.6210559010505676}\n",
      "Round: 2 {'accuracy': 0.58365398645401}\n",
      "Round: 3 {'accuracy': 0.602124035358429}\n",
      "Round: 4 {'accuracy': 0.6253655552864075}\n",
      "Round: 5 {'accuracy': 0.6389102935791016}\n",
      "Round: 6 {'accuracy': 0.633061408996582}\n",
      "Round: 7 {'accuracy': 0.633061408996582}\n",
      "Round: 8 {'accuracy': 0.633061408996582}\n",
      "Round: 9 {'accuracy': 0.633677065372467}\n",
      "Round: 10 {'accuracy': 0.6344466805458069}\n",
      "Round: 11 {'accuracy': 0.6355240941047668}\n",
      "Round: 12 {'accuracy': 0.6350623369216919}\n",
      "Round: 13 {'accuracy': 0.6350623369216919}\n",
      "Round: 14 {'accuracy': 0.634754478931427}\n",
      "Round: 15 {'accuracy': 0.635216236114502}\n",
      "Round: 16 {'accuracy': 0.634754478931427}\n",
      "Round: 17 {'accuracy': 0.6350623369216919}\n",
      "Round: 18 {'accuracy': 0.6356779932975769}\n",
      "Round: 19 {'accuracy': 0.6358319520950317}\n",
      "Round: 20 {'accuracy': 0.6366015076637268}\n",
      "Round: 21 {'accuracy': 0.6364476084709167}\n",
      "Round: 22 {'accuracy': 0.6376789212226868}\n",
      "Round: 23 {'accuracy': 0.6381406784057617}\n",
      "Round: 24 {'accuracy': 0.6387563347816467}\n",
      "Round: 25 {'accuracy': 0.6392180919647217}\n",
      "Round: 26 {'accuracy': 0.6399877071380615}\n",
      "Round: 27 {'accuracy': 0.6398337483406067}\n",
      "Round: 28 {'accuracy': 0.6407572627067566}\n",
      "Round: 29 {'accuracy': 0.6421425342559814}\n",
      "Round: 30 {'accuracy': 0.6422964334487915}\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "async with connect(\n",
    "    project_id=PROJECT_ID,\n",
    "    organization_id=ORGANIZATION_ID,\n",
    "    server_address=SERVER_ADDRESS,\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    token_url=TOKEN_URL,\n",
    ") as session:\n",
    "    job_id = await session.fit(job_spec)  # noqa: PLE1142\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print(f\"https://{SERVER_ADDRESS}/{ORGANIZATION_ID}/{PROJECT_ID}/jobs\")\n",
    "    round = 1\n",
    "    async for metrics in session.metrics(job_id):\n",
    "        print(\"Round:\", round, {m.name: metrics.items[f\"val_{m.name}\"] for m in job_spec.config.metrics})\n",
    "        round += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen from the training that the model wasn't able to learn the data. We possibily made a mistake in the data preperation. Therefore, we will go back and check the pipeline. We will change the aggregation function from `MAX` to `MEAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:31.601779Z",
     "iopub.status.busy": "2024-07-04T12:18:31.599720Z",
     "iopub.status.idle": "2024-07-04T12:18:31.616407Z",
     "shell.execute_reply": "2024-07-04T12:18:31.614502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adjust the pipeline to use average instead of max aggregation\n",
    "# fmt: off\n",
    "pipeline = (\n",
    "    Source(datasets[DATASET_NAMES[0]])\n",
    "    | GroupBy([\"id\"], [Alias(Avg(f), f) for f in fields + target])\n",
    "    + Targets(Select(target) + Cast(target, CastType.Float32))\n",
    "    + Features(Select(fields) + Cast(fields, CastType.Float32) + MinMaxScaler(fields))\n",
    ")\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new Job with the updated pipeline and start the training. This time the model should be able to learn the data and we should see the accuracy increasing over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:31.622179Z",
     "iopub.status.busy": "2024-07-04T12:18:31.621744Z",
     "iopub.status.idle": "2024-07-04T12:18:31.646218Z",
     "shell.execute_reply": "2024-07-04T12:18:31.645066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the job spec\n",
    "job_spec = build_job_spec(\n",
    "    name=\"chemical\",\n",
    "    pipeline=pipeline,\n",
    "    model=model_from_torch(MODEL),\n",
    "    config=JobSpecConfig(\n",
    "        num_rounds=30,\n",
    "        batch_size=256,\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        loss_function=BinaryCrossEntropyWLogitsLoss(),\n",
    "        metrics=[BinaryAccuracy(threshold=0.0)],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T12:18:31.649238Z",
     "iopub.status.busy": "2024-07-04T12:18:31.649012Z",
     "iopub.status.idle": "2024-07-04T12:18:57.735290Z",
     "shell.execute_reply": "2024-07-04T12:18:57.731206Z"
    },
    "tags": [
     "submit_job"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-07-15 08:45:48\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mIssuing new access token      \u001b[0m \u001b[36msource\u001b[0m=\u001b[35mkatulu.core.auth.jwt\u001b[0m\n",
      "Job ID: 80c6eec9-ed3b-471a-a78c-fc06597700fb\n",
      "https://https://platform.katulu.io/ca2bf2b8-3cd1-427b-8b39-c1b09c72c0ed/bcd538b2-422f-400e-a9fd-18aa54f92ec5/jobs\n",
      "Round: 1 {'accuracy': 0.6596890687942505}\n",
      "Round: 2 {'accuracy': 0.6596890687942505}\n",
      "Round: 3 {'accuracy': 0.7077112793922424}\n",
      "Round: 4 {'accuracy': 0.7175619602203369}\n",
      "Round: 5 {'accuracy': 0.7175619602203369}\n",
      "Round: 6 {'accuracy': 0.734184980392456}\n",
      "Round: 7 {'accuracy': 0.7394182085990906}\n",
      "Round: 8 {'accuracy': 0.7394182085990906}\n",
      "Round: 9 {'accuracy': 0.7452670335769653}\n",
      "Round: 10 {'accuracy': 0.7460366487503052}\n",
      "Round: 11 {'accuracy': 0.7460366487503052}\n",
      "Round: 12 {'accuracy': 0.7494227886199951}\n",
      "Round: 13 {'accuracy': 0.7529628872871399}\n",
      "Round: 14 {'accuracy': 0.7529628872871399}\n",
      "Round: 15 {'accuracy': 0.7540403008460999}\n",
      "Round: 16 {'accuracy': 0.7541942596435547}\n",
      "Round: 17 {'accuracy': 0.7541942596435547}\n",
      "Round: 18 {'accuracy': 0.7583500146865845}\n",
      "Round: 19 {'accuracy': 0.7591195702552795}\n",
      "Round: 20 {'accuracy': 0.7591195702552795}\n",
      "Round: 21 {'accuracy': 0.7594274282455444}\n",
      "Round: 22 {'accuracy': 0.7597352862358093}\n",
      "Round: 23 {'accuracy': 0.7597352862358093}\n",
      "Round: 24 {'accuracy': 0.7594274282455444}\n",
      "Round: 25 {'accuracy': 0.7601970434188843}\n",
      "Round: 26 {'accuracy': 0.7601970434188843}\n",
      "Round: 27 {'accuracy': 0.7601970434188843}\n",
      "Round: 28 {'accuracy': 0.7598891854286194}\n",
      "Round: 29 {'accuracy': 0.7600430846214294}\n",
      "Round: 30 {'accuracy': 0.7606587409973145}\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "async with connect(\n",
    "    project_id=PROJECT_ID,\n",
    "    organization_id=ORGANIZATION_ID,\n",
    "    server_address=SERVER_ADDRESS,\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    token_url=TOKEN_URL,\n",
    ") as session:\n",
    "    job_id = await session.fit(job_spec)  # noqa: PLE1142\n",
    "    print(f\"Job ID: {job_id}\")\n",
    "    print(f\"https://{SERVER_ADDRESS}/{ORGANIZATION_ID}/{PROJECT_ID}/jobs\")\n",
    "    round = 1\n",
    "    async for metrics in session.metrics(job_id):\n",
    "        print(\"Round:\", round, {m.name: metrics.items[f\"val_{m.name}\"] for m in job_spec.config.metrics})\n",
    "        round += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that the Accuracy increased by more then 10% and the model is able to predict the quality of the chemical solution with an accuracy of around 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the demo we have seen how to install and onboard agents, how to access the data and prepare it for training, how to train a model using federated learning and how to evaluate the model. Now it is time to explore the platform and try it out yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}